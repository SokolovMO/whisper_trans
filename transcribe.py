#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "torch>=2.6.0",
#   "torchvision>=0.21.0",
#   "openai-whisper>=20240930",
# ]
# [tool.uv.sources]
# torch = [
#   { index = "pytorch-cu124", marker = "sys_platform == 'win32'" },
# ]
# torchvision = [
#   { index = "pytorch-cu124", marker = "sys_platform == 'win32'" },
# ]
# [[tool.uv.index]]
# name = "pytorch-cpu"
# url = "https://download.pytorch.org/whl/cpu"
# explicit = true
# [[tool.uv.index]]
# name = "pytorch-cu124"
# url = "https://download.pytorch.org/whl/cu124"
# explicit = true
# ///

import whisper
from pathlib import Path
from datetime import datetime
import sys


def select_model_interactively():
    models = ["tiny", "base", "small", "medium", "large"]
    print("ðŸ“¦ Ð²Ñ‹Ð±ÐµÑ€Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Whisper:\n")
    for i, name in enumerate(models, start=1):
        print(f"{i}. {name}")
    try:
        choice = int(input("\n Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð½Ð¾Ð¼ÐµÑ€ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (1-5): "))
        if 1 <= choice <= 5:
            return models[choice - 1]
        else:
            print("âš ï¸ Ð½ÐµÐ²ÐµÑ€Ð½Ñ‹Ð¹ Ð²Ñ‹Ð±Ð¾Ñ€, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ: base")
            return "base"
    except Exception as e:
        print(
            f"âš ï¸ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ñ€Ð¾Ñ‡Ð¸Ñ‚Ð°Ñ‚ÑŒ Ð²Ñ‹Ð±Ð¾Ñ€, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ: base. {e}"
        )
        return "base"


def transcribe_audio(file_path: Path, model_name="base"):
    print(f"\nðŸ“‚ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð»: {file_path}")
    if not file_path.exists():
        print(f"âŒ Ñ„Ð°Ð¹Ð» Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: {file_path}")
        return

    try:
        print(f"ðŸ“¦ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ whisper: {model_name}")
        model = whisper.load_model(model_name)

        print("ðŸ§  Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð±Ð°Ñ†Ð¸ÑŽ...")
        result = model.transcribe(str(file_path))

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = file_path.parent / "results_transcribe"
        output_dir.mkdir(exist_ok=True)
        output_plain = output_dir / f"{file_path.stem}_{timestamp}_transcription.txt"
        output_timed = output_dir / f"{file_path.stem}_{timestamp}_with_timestamps.txt"

        with open(output_plain, "w", encoding="utf-8") as f:
            f.write(result["text"])

        def format_timestamp(seconds: float) -> str:
            total_seconds = int(seconds)
            hours = total_seconds // 3600
            mins = (total_seconds % 3600) // 60
            secs = total_seconds % 60
            return f"{hours:02}:{mins:02}:{secs:02}"


        with open(output_timed, "w", encoding="utf-8") as f:
            for segment in result.get("segments", []):
                start = format_timestamp(segment['start'])
                end = format_timestamp(segment['end'])
                f.write(f"[{start} - {end}] {segment['text']}\n")


        print(
            f"âœ… Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð±Ð°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°.\nðŸ“ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾:\n- {output_plain.name}\n- {output_timed.name}"
        )

    except Exception as e:
        print(f"âŒ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {e}")


def main():
    args = sys.argv[1:]

    if not args:
        print("â— Ð¿ÐµÑ€ÐµÑ‚Ð°Ñ‰Ð¸ Ð°ÑƒÐ´Ð¸Ð¾Ñ„Ð°Ð¹Ð» Ð½Ð° ÑÐºÑ€Ð¸Ð¿Ñ‚, Ð¸Ð»Ð¸ Ð¿ÐµÑ€ÐµÐ´Ð°Ð¹ Ð¿ÑƒÑ‚ÑŒ Ð² Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ…\n")
        return

    file_path = Path(args[0])

    if len(args) > 1:
        model_name = args[1]
    else:
        model_name = select_model_interactively()

    transcribe_audio(file_path, model_name=model_name)


if __name__ == "__main__":
    main()
